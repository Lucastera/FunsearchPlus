# !git clone https://github.com/RayZhhh/funsearch.git

import argparse
import os
import random
import sys

from log_prompt import log_prompt_response

sys.path.append('/content/funsearch/')

import time
import json
import multiprocessing
from typing import Collection, Any
import http.client
from implementation import sampler


def generate_log_dir(dataset_name, strategies, multi_enabled=False, dup_check_enabled=False, dup_method="similarity"):
    """Generate unique log directory name for different configurations
    
    Args:
        dataset_name: Dataset name ('weibull' or 'OR3')
        strategies: List of strategies used
        multi_enabled: Whether multi-strategy is enabled
        dup_check_enabled: Whether duplicate check is enabled
        dup_method: Duplicate check method
    
    Returns:
        str: Unique log directory path
    """
    strategy_str = "_".join(strategies)
    multi_part = "multi" if multi_enabled else "single"
    dup_part = "nodup" if not dup_check_enabled else f"dup_{dup_method}"
    
    return f"./logs/funsearch_{dataset_name}_{strategy_str}_{multi_part}_{dup_part}"

def _trim_preface_of_body(sample: str) -> str:
    """Trim the redundant descriptions/symbols/'def' declaration before the function body.
    Please see my comments in sampler.LLM (in sampler.py).
    Since the LLM used in this file is not a pure code completion LLM, this trim function is required.

    -Example sample (function & description generated by LLM):
    -------------------------------------
    This is the optimized function ...
    def priority_v2(...) -> ...:
        return ...
    This function aims to ...
    -------------------------------------
    -This function removes the description above the function's signature, and the function's signature.
    -The indent of the code is preserved.
    -Return of this function:
    -------------------------------------
        return ...
    This function aims to ...
    -------------------------------------
    """
    lines = sample.splitlines()
    func_body_lineno = 0
    find_def_declaration = False
    for lineno, line in enumerate(lines):
        # find the first 'def' statement in the given code
        if line[:3] == 'def':
            func_body_lineno = lineno
            find_def_declaration = True
            break
    if find_def_declaration:
        code = ''
        for line in lines[func_body_lineno + 1:]:
            code += line + '\n'
        return code
    return sample


# Add class variable to store log directory
class LLMAPI(sampler.LLM):
    """Language model that predicts continuation of provided source code.
    """
    _log_dir = None
    def __init__(self, samples_per_prompt: int, multi_strategy_config=None, trim=True):
        super().__init__(samples_per_prompt, multi_strategy_config)
        base_prompt = ('Complete a different and more complex Python function. '
                       'Be creative and you can insert multiple if-else and for-loop in the code logic.'
                       'Only output the Python code, no descriptions.')
        # self._multi_additional_prompt = 'Complete a different Python function.'
        # self._multi_additional_prompt_suffix = 'Only output the Python code, no descriptions.'
        # self._evolutionary_perspectives = [
        #     "Consider the problem from a different angle than conventional approaches",
        #     "Think about a novel representation of the problem elements",
        #     "Explore an adaptive strategy that responds to input characteristics",
        #     "Consider what nature-inspired processes might solve this problem efficiently",
        #     "Think about how this problem could be decomposed differently"
        # ]
        self._additional_prompt = base_prompt
        self._trim = trim

    def draw_samples(self, prompt: str) -> Collection[str]:
        """Returns multiple predicted continuations of `prompt`."""
        return [self._draw_sample(prompt) for _ in range(self._samples_per_prompt)]

    def _draw_sample(self, content: str) -> str:
        """Get strategy-specific prompt and generate code sample."""
        strategy_prompt, selected_strategies = self._get_strategy_prompt()
        
        # Combine base prompt with strategy-specific guidance
        if strategy_prompt:
            # random_perspective = random.choice(self._evolutionary_perspectives)
            # enhanced_multi_prompt = f"{self._multi_additional_prompt} {random_perspective}. {self._multi_additional_prompt_suffix}"
            # prompt_text = f'\n'.join([content, f"{enhanced_multi_prompt} {strategy_prompt}"])
            prompt_text = f'\n'.join([content, f"{self._additional_prompt} {strategy_prompt}"])            
        else:
            prompt_text = '\n'.join([content, self._additional_prompt])

        while True:
            try:
                conn = http.client.HTTPSConnection("api.zhizengzeng.com")
                payload = json.dumps({
                    "max_tokens": 512,
                    "model": "gpt-3.5-turbo",
                    "messages": [
                        {
                            "role": "user",
                            "content": prompt_text
                        }
                    ]
                })
                headers = {
                    'Authorization': 'Bearer sk-zk2ab5e237c4f881fb0bd6946884ab85136674377293100e',
                    'User-Agent': 'Apifox/1.0.0 (https://apifox.com)',
                    'Content-Type': 'application/json'
                }
                conn.request("POST", "/v1/chat/completions", payload, headers)
                res = conn.getresponse()
                data = res.read().decode("utf-8")
                data = json.loads(data)
                response = data['choices'][0]['message']['content']
                
                log_file = f"{self.__class__._log_dir}/prompt_response_log.jsonl" if self.__class__._log_dir else './logs/funsearch_llm_test/prompt_response_log.jsonl'
                log_prompt_response(prompt_text, response, selected_strategies, log_file)

                # trim function
                if self._trim:
                    response = _trim_preface_of_body(response)
                return response
            except Exception:
                time.sleep(1)
                continue

from implementation import evaluator
from implementation import evaluator_accelerate


class Sandbox(evaluator.Sandbox):
    """Sandbox for executing generated code. Implemented by RZ.

    RZ: Sandbox returns the 'score' of the program and:
    1) avoids the generated code to be harmful (accessing the internet, take up too much RAM).
    2) stops the execution of the code in time (avoid endless loop).
    """

    def __init__(self, verbose=False, numba_accelerate=True):
        """
        Args:
            verbose         : Print evaluate information.
            numba_accelerate: Use numba to accelerate the evaluation. It should be noted that not all numpy functions
                              support numba acceleration, such as np.piecewise().
        """
        self._verbose = verbose
        self._numba_accelerate = numba_accelerate

    def run(
            self,
            program: str,
            function_to_run: str,  # RZ: refers to the name of the function to run (e.g., 'evaluate')
            function_to_evolve: str,  # RZ: accelerate the code by decorating @numba.jit() on function_to_evolve.
            inputs: Any,  # refers to the dataset
            test_input: str,  # refers to the current instance
            timeout_seconds: int,
            **kwargs  # RZ: add this
    ) -> tuple[Any, bool]:
        """Returns `function_to_run(test_input)` and whether execution succeeded.

        RZ: If the generated code (generated by LLM) is executed successfully,
        the output of this function is the score of a given program.
        RZ: PLEASE NOTE THAT this SandBox is only designed for bin-packing problem.
        """
        dataset = inputs[test_input]
        try:
            result_queue = multiprocessing.Queue()
            process = multiprocessing.Process(
                target=self._compile_and_run_function,
                args=(program, function_to_run, function_to_evolve, dataset, self._numba_accelerate, result_queue)
            )
            process.start()
            process.join(timeout=timeout_seconds)
            if process.is_alive():
                # if the process is not finished in time, we consider the program illegal
                process.terminate()
                process.join()
                results = None, False
            else:
                if not result_queue.empty():
                    results = result_queue.get_nowait()
                else:
                    results = None, False

            return results
        except:
            return None, False

    def _compile_and_run_function(self, program, function_to_run, function_to_evolve, dataset, numba_accelerate,
                                  result_queue):
        try:
            # optimize the code (decorate function_to_run with @numba.jit())
            if numba_accelerate:
                program = evaluator_accelerate.add_numba_decorator(
                    program=program,
                    function_to_evolve=function_to_evolve
                )
            # compile the program, and maps the global func/var/class name to its address
            all_globals_namespace = {}
            # execute the program, map func/var/class to global namespace
            exec(program, all_globals_namespace)
            # get the pointer of 'function_to_run'
            function_to_run = all_globals_namespace[function_to_run]
            # return the execution results
            results = function_to_run(dataset)
            # the results must be int or float
            if not isinstance(results, (int, float)):
                result_queue.put((None, False))
                return
            result_queue.put((results, True))
        except Exception:
            # if raise any exception, we assume the execution failed
            result_queue.put((None, False))

specification = r'''
import numpy as np


def get_valid_bin_indices(item: float, bins: np.ndarray) -> np.ndarray:
    """Returns indices of bins in which item can fit."""
    return np.nonzero((bins - item) >= 0)[0]


def online_binpack(
        items: tuple[float, ...], bins: np.ndarray
) -> tuple[list[list[float, ...], ...], np.ndarray]:
    """Performs online binpacking of `items` into `bins`."""
    # Track which items are added to each bin.
    packing = [[] for _ in bins]
    # Add items to bins.
    for item in items:
        # Extract bins that have sufficient space to fit item.
        valid_bin_indices = get_valid_bin_indices(item, bins)
        # Score each bin based on heuristic.
        priorities = priority(item, bins[valid_bin_indices])
        # Add item to bin with highest priority.
        best_bin = valid_bin_indices[np.argmax(priorities)]
        bins[best_bin] -= item
        packing[best_bin].append(item)
    # Remove unused bins from packing.
    packing = [bin_items for bin_items in packing if bin_items]
    return packing, bins


@funsearch.run
def evaluate(instances: dict) -> float:
    """Evaluate heuristic function on a set of online binpacking instances."""
    # List storing number of bins used for each instance.
    num_bins = []
    # Perform online binpacking for each instance.
    for name in instances:
        instance = instances[name]
        capacity = instance['capacity']
        items = instance['items']
        # Create num_items bins so there will always be space for all items,
        # regardless of packing order. Array has shape (num_items,).
        bins = np.array([capacity for _ in range(instance['num_items'])])
        # Pack items into bins and return remaining capacity in bins_packed, which
        # has shape (num_items,).
        _, bins_packed = online_binpack(items, bins)
        # If remaining capacity in a bin is equal to initial capacity, then it is
        # unused. Count number of used bins.
        num_bins.append((bins_packed != capacity).sum())
    # Score of heuristic function is negative of average number of bins used
    # across instances (as we want to minimize number of bins).
    return -np.mean(num_bins)


@funsearch.evolve
def priority(item: float, bins: np.ndarray) -> np.ndarray:
    """Returns priority with which we want to add item to each bin.

    Args:
        item: Size of item to be added to the bin.
        bins: Array of capacities for each bin.

    Return:
        Array of same size as bins with priority score of each bin.
    """
    ratios = item / bins
    log_ratios = np.log(ratios)
    priorities = -log_ratios
    return priorities
'''

import bin_packing_utils

bin_packing_or3 = {'OR3': bin_packing_utils.datasets['OR3']}

from implementation import funsearch
from implementation import config

# It should be noted that the if __name__ == '__main__' is required.
# Because the inner code uses multiprocess evaluation.
def run_OR3():
    # 記錄開始時間
    start_time = time.time()
    class_config = config.ClassConfig(llm_class=LLMAPI, sandbox_class=Sandbox)
    config_params = config.Config(samples_per_prompt=4)
    global_max_sample_num = 20  # if it is set to None, funsearch will execute an endless loop
    
    bin_packing_or3 = {'OR3': bin_packing_utils.datasets['OR3']}
    
    funsearch.main(
        specification=specification,
        inputs=bin_packing_or3,
        config=config_params,
        max_sample_nums=global_max_sample_num,
        class_config=class_config,
        log_dir='./logs/funsearch_llm_original',
        enable_duplicate_check=True,
        duplicate_check_method='similarity',  # 'hash' or 'similarity' or 'ai_agent'
        similarity_threshold=0.8,  # only works when duplicate_check_method='similarity' or 'ai_agent'
        enable_multi_strategy=True,  # 控制是否启用多策略优化
        multi_num=2,  # 本次多目标每次选择几个目标
        multi_strategies=["algorithm", "code_structure", "python_features"]  # 启用的策略列表
    )
    # 記錄結束時間
    end_time = time.time()

    # 計算並打印所用時間
    elapsed_time = end_time - start_time
    print(f"Funsearch 執行完成，所用時間: {elapsed_time:.2f} 秒")

def run_weibull():
    # 記錄開始時間
    start_time = time.time()
    class_config = config.ClassConfig(llm_class=LLMAPI, sandbox_class=Sandbox)
    config_params = config.Config(samples_per_prompt=4)
    global_max_sample_num = 2000  # if it is set to None, funsearch will execute an endless loop
    
    bin_packing_weibull = {'weibull': bin_packing_utils.datasets['Weibull 5k']}
    
    funsearch.main(
        specification=specification,
        inputs=bin_packing_weibull,
        config=config_params,
        max_sample_nums=global_max_sample_num,
        class_config=class_config,
        log_dir='./logs/funsearch_llm_original_weibull',
        enable_duplicate_check=False,
        duplicate_check_method='similarity',  # 'hash' or 'similarity' or 'ai_agent'
        similarity_threshold=0.8,  # only works when duplicate_check_method='similarity' or 'ai_agent'
        enable_multi_strategy=False,  # 控制是否启用多策略优化
        multi_num=1,  # 本次多目标每次选择几个目标
        # multi_strategies=["quality", "code_structure"]  # 启用的策略列表
        multi_strategies=["algorithm"]
        # multi_strategies=["quality"]  # 启用的策略列表
    )
    # 記錄結束時間
    end_time = time.time()

    # 計算並打印所用時間
    elapsed_time = end_time - start_time
    print(f"Funsearch 執行完成，所用時間: {elapsed_time:.2f} 秒")
    
# if __name__ == '__main__':
#     # run_OR3()
#     run_weibull()

def run_experiment(dataset='weibull', strategies=["algorithm"], 
                  enable_multi=False, multi_num=1, 
                  enable_dup_check=False, dup_method='similarity',
                  max_samples=2000):
    """Run experiment with specified configuration"""
    # Record start time
    start_time = time.time()
    
    # Create unique log directory
    log_dir = generate_log_dir(
        dataset_name=dataset, 
        strategies=strategies, 
        multi_enabled=enable_multi,
        dup_check_enabled=enable_dup_check,
        dup_method=dup_method
    )
    os.makedirs(log_dir, exist_ok=True)
    
    # Record experiment configuration
    config_info = {
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S", time.localtime()),
        "command": " ".join(sys.argv),
        "configuration": {
            "dataset": dataset,
            "strategies": strategies,
            "enable_multi_strategy": enable_multi,
            "multi_num": multi_num,
            "enable_duplicate_check": enable_dup_check,
            "duplicate_check_method": dup_method,
            "max_samples": max_samples
        }
    }
    
    with open(f"{log_dir}/experiment_config.json", 'w') as f:
        json.dump(config_info, f, indent=2)
    
    # Set up configurations
    class_config = config.ClassConfig(llm_class=LLMAPI, sandbox_class=Sandbox)
    config_params = config.Config(samples_per_prompt=4)
    
    # Set dataset
    if dataset.lower() == 'weibull':
        dataset_dict = {'weibull': bin_packing_utils.datasets['Weibull 5k']}
    elif dataset.lower() == 'or3':
        dataset_dict = {'OR3': bin_packing_utils.datasets['OR3']}
    else:
        raise ValueError(f"Unknown dataset: {dataset}")
    
    # Set LLM log directory
    LLMAPI._log_dir = log_dir
    
    # Run FunSearch
    funsearch.main(
        specification=specification,
        inputs=dataset_dict,
        config=config_params,
        max_sample_nums=max_samples,
        class_config=class_config,
        log_dir=log_dir,
        enable_duplicate_check=enable_dup_check,
        duplicate_check_method=dup_method,
        similarity_threshold=0.8,
        enable_multi_strategy=enable_multi,
        multi_num=multi_num,
        multi_strategies=strategies
    )
    
    # Record end time and runtime
    end_time = time.time()
    elapsed_time = end_time - start_time
    
    # Update configuration file with runtime information
    config_info["runtime"] = {
        "seconds": elapsed_time,
        "formatted": f"{elapsed_time:.2f} seconds"
    }
    
    with open(f"{log_dir}/experiment_config.json", 'w') as f:
        json.dump(config_info, f, indent=2)
    
    print(f"Funsearch execution completed, runtime: {elapsed_time:.2f} seconds")
    
    return log_dir

if __name__ == '__main__':
    # Parse command line arguments
    parser = argparse.ArgumentParser(description='Run FunSearch experiment')
    parser.add_argument('--dataset', type=str, default='weibull', choices=['weibull', 'OR3'], 
                      help='Dataset to use')
    parser.add_argument('--strategies', type=str, nargs='+', 
                      default=[],
                      choices=['algorithm', 'code_structure', 'python_features', 'quality', 'mathematical_optimization'],
                      help='List of strategies to use')
    parser.add_argument('--multi_num', type=int, default=1, 
                      help='Number of strategies to use each time')
    parser.add_argument('--enable_multi', action='store_true', default=False,
                      help='Enable multi-strategy optimization')
    parser.add_argument('--enable_dup_check', action='store_true', default=False,
                      help='Enable duplicate check')
    parser.add_argument('--dup_method', type=str, default='similarity', 
                      choices=['hash', 'similarity', 'ai_agent'],
                      help='Duplicate check method')
    parser.add_argument('--max_samples', type=int, default=1000,
                      help='Maximum number of samples')
    
    args = parser.parse_args()
    
    # Run experiment
    log_dir = run_experiment(
        dataset=args.dataset,
        strategies=args.strategies,
        enable_multi=args.enable_multi,
        multi_num=args.multi_num,
        enable_dup_check=args.enable_dup_check,
        dup_method=args.dup_method,
        max_samples=args.max_samples
    )
    
    print(f"Experiment results saved in: {log_dir}")
    print(f"Use the following command to view TensorBoard: tensorboard --logdir {log_dir}")